## 语言模型

语言模型是计算一个“句子”有多大可能真的是一个句子的概率的模型，也就是用来判断一句话是否合理的模型。当我们利用输入法输入一整句话，或者是利用搜索引擎时看到的自动补全功能，全都是语言模型的功劳。

历史上，人们最开始尝试开发的语言模型是基于**规则**的语言模型，即通过一种语言具体的语法等规则来计算一个“句子”成立的概率，但这种方式遇到了诸多问题，且进展十分困难。后来，有人发明了基于**统计**的语言模型，实践证明这种语言模型的效果非常好。所谓的 N-Gram 模型，就是一种基于统计的语言模型。

随着深度学习的发展，现在还出现了诸如基于 RNN 等技术建立的神经网络语言模型，效果比 N-Gram 更好一些，但是基于 RNN 的解码要求每次都要现场对概率进行计算，在某些时候不能满足实时性的要求，因此具体使用哪种技术还需要根据具体的需求进行判断。

## N-Gram 模型

一个由 n 个词组成的句子 S 确实是一句话的概率是：
（待补图）
对于一句具体的话而言，其词组之间是有先后顺序的，因此可以通过条件概率计算这句话的概率，即：
（待补图）
然而，这样计算句子的概率，会导致其中任何一个词都需要考虑其前面所有的词，这即难以计算，又没有多大的意义，因此，我们需要对这个情况加以简化。

### 马尔科夫假设

对概率简化的手段是引入**马尔科夫假设**。所谓马尔科夫假设，是指每个词出现的概率只与其前面紧密相连的若干词相关，如一阶马尔科夫假设是只考虑当前词前面的一个词，对应的语言模型是二元模型；二阶的马尔科夫假设只考虑当前词前面的两个词，对应的语言模型是三元模型。

马尔科夫假设的理论基础是[马尔科夫链 (Markov chain)](https://en.wikipedia.org/wiki/Markov_chain)。
>马尔科夫链表征状态空间中从一个状态到另一个状态转换的随机过程。该过程要求“无记忆”性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。

### 简化的概率模型

引入马尔科夫假设后，一个句子成立的概率变成了：
（待补图）

>*注：这里的 m 表示与前 m 个词相关*

从而，当 m=1，2，3，...，我们便得到了对应的一元模型，二元模型，三元模型了，即：
* m=1时，一元模型为：
（待补图）
* m=2时，二元模型为：
（待补图）
* m=3时，三元模型为：
（待补图）

依次类推，令 m=N，称这种语言模型为 N-Gram 模型。
